# NLP_ProbabilisticModels

Context: I completed these guided programming exercises during the Coursera course entitled Natural Language Processing with Probabilistic Models.

1. C2_W1_Assignment.ipynb:
This notebook records the steps taken to build an autocorrect system based on word probabilities and presents a dynamic programming system for calculating the minimum edit distance between two strings.

2. C2_W2_Assignment.ipynb:
This notebook presents a hidden Markov model for part-of-speech tagging in terms of a transmission matrix and an emission matrix, both constructed from a corpus with smoothing. Then, it illustrates how the Viterbi algorithm can be used to predict the most likely sequence of parts of speech for a sentence.

3. C2_W3_Assignment.ipynb:
This notebook records the procedures involved in building an auto-complete system based on the n-grams language model and the perplexity score.

4. C2_W4_Assignment.ipynb:
This notebook explains how the continuous bag of words (CBOW) model can be used to generate word embeddings from a corpus.
